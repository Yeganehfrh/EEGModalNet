{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import mne\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler, robust_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from data/test/sub-032301/RSEEG/sub-010002.vhdr...\n",
      "Setting channel info structure...\n"
     ]
    }
   ],
   "source": [
    "# check the current working directory\n",
    "cwd = '/Users/yeganeh/Codes/EEGNet/'\n",
    "if os.getcwd() != cwd:\n",
    "    os.chdir(cwd)\n",
    "\n",
    "# channel names\n",
    "EEG = mne.io.read_raw_brainvision('data/test/sub-032301/RSEEG/sub-010002.vhdr', eog=['VEOG'])\n",
    "ch_names = EEG.info['ch_names']\n",
    "ch_names.remove('VEOG')\n",
    "del EEG\n",
    "\n",
    "# helper function to load the data\n",
    "# Helper functions\n",
    "# clamp\n",
    "def clamp_eeg(data, axis=1, deviation=20):\n",
    "    \"\"\"Clamp EEG data proportional to the standard deviation of each channel.\"\"\"\n",
    "    # container for clamped data\n",
    "    data_clamped = np.zeros_like(data)\n",
    "    upper_dev = data.mean(axis=axis) + data.std(axis=axis) * deviation\n",
    "    lower_dev = data.mean(axis=axis) - data.std(axis=axis) * deviation\n",
    "    for i in range(data.shape[0]):\n",
    "        data_clamped[i, :] = np.clip(data[i, :], lower_dev[i], upper_dev[i])\n",
    "    return data_clamped\n",
    "\n",
    "def print_min_max(data, lower_dev, upper_dev, sub, channel):\n",
    "    \"\"\"Prints the min and max of a channel and checks if it would be clamped by the clamp_eeg function\n",
    "    This is used to check if the clamp function is working as expected.\"\"\"\n",
    "    if data[sub, channel, :].min() < lower_dev[sub, channel] or data[sub, channel, :].max() > upper_dev[sub, channel]:\n",
    "        print(f'Data: {sub}, {channel}: Min={data[sub, channel, :].min()}, Max={data[sub, channel, :].max()}',\n",
    "              f'Bounderies: Lower={lower_dev[sub, channel]}, Higher={upper_dev[sub, channel]}')\n",
    "\n",
    "def power_to_db(S, ref=1.0, amin=1e-10, top_db=80.0):\n",
    "    ref_value = torch.tensor(ref)\n",
    "    amin = torch.tensor(amin)\n",
    "    log_spec = 10.0 * torch.log10(torch.maximum(amin, S))\n",
    "    log_spec -= 10.0 * torch.log10(torch.maximum(amin, ref_value))\n",
    "\n",
    "    if top_db is not None:\n",
    "        if top_db < 0:\n",
    "            raise ValueError(\"top_db must be non-negative\")\n",
    "        log_spec = torch.maximum(log_spec, log_spec.max() - top_db)\n",
    "\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = xr.open_dataarray('data/baseline_corrected/eeg_EC_BaseCorr.nc5', engine='h5netcdf')\n",
    "subjects = data.subject.values[:50]\n",
    "\n",
    "# an xarray container for collecting procesed data across subjects\n",
    "processed_data = {}\n",
    "\n",
    "for sub in subjects:\n",
    "  print(f'Processing {sub}')\n",
    "  sub_data = data.sel(subject=sub).values\n",
    "\n",
    "  # normalize\n",
    "  sub_data = RobustScaler().fit_transform(sub_data)\n",
    "\n",
    "  # clamp\n",
    "  sub_data = clamp_eeg(sub_data)\n",
    "\n",
    "  processed_data[sub] = sub_data\n",
    "\n",
    "# save the processed data\n",
    "data = xr.DataArray(np.array(list(processed_data.values())),\n",
    "                    dims=('subject', 'channel', 'time'),\n",
    "                    coords={'subject': list(processed_data.keys()),\n",
    "                            'channel': ch_names})\n",
    "# data.to_netcdf('data/processed/normalized_clamped/eeg_EC_BaseCorr_Norm_Clamp_50.nc5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "#### time-frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open processed eeg data\n",
    "data = xr.open_dataarray('data/processed/normalized_clamped/eeg_EC_BaseCorr_Norm_Clamp_50.nc5', engine='h5netcdf')\n",
    "subjects = data.subject.values\n",
    "\n",
    "tfrs = {}\n",
    "\n",
    "# morlet wavelet transform\n",
    "freqs = np.arange(7, 30, 2) \n",
    "n_cycles = [np.ceil(i) for i in freqs/(freqs[0]-2)]\n",
    "\n",
    "for sub in subjects:\n",
    "    data = data.sel(subject=sub).values\n",
    "    data = np.expand_dims(data, axis=0)  # add an additional dimension to the data\n",
    "    tfr = mne.time_frequency.tfr_array_morlet(data, freqs=freqs, sfreq=128, n_cycles=n_cycles,\n",
    "                                              output='power',\n",
    "                                              n_jobs=-1)\n",
    "    tfrs[sub] = tfr\n",
    "\n",
    "# convert the dictionary to a xarray\n",
    "tfrs = xr.DataArray(tfrs)\n",
    "tfrs = tfrs.rename({'dim_0': 'subject', 'dim_1': 'frequency', 'dim_2': 'channel', 'dim_3': 'time'})\n",
    "tfrs = tfrs.transpose('subject', 'frequency', 'channel', 'time')\n",
    "tfrs.to_netcdf('data/tfrs.nc5')\n",
    "\n",
    "# maybe only the spatial and frequency dimensions are important, so we can average over the time dimension\n",
    "# and plot the heatmap of the average power over time (frequency X channels)\n",
    "# Because our data is not time-locked!\n",
    "tfr = tfr.squeeze().mean(axis=-1)\n",
    "tfr = power_to_db(torch.tensor(tfr))\n",
    "\n",
    "# plot heatmap of the average power over time (frequency x channels)\n",
    "plt.imshow(tfr, aspect='auto', origin='lower', cmap='jet')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "# plt.colorbar()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig('data/figures/tfr_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot topomap and use that as the input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a epochs object\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=128, ch_types='eeg')\n",
    "epochs = mne.EpochsArray(data, info)\n",
    "\n",
    "pos = mne.channels.make_standard_montage('standard_1005')\n",
    "# add digitization points to the epoched data\n",
    "epochs.set_montage(pos)\n",
    "mne.viz.plot_tfr_topomap(tfr[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
